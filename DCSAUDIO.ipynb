{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1d2c4d9-38fd-4390-90c8-6ca2ab7b9116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (0.8.1)\n",
      "Requirement already satisfied: scipy in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (1.5.4)\n",
      "Requirement already satisfied: ipywidgets in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (8.1.7)\n",
      "Requirement already satisfied: matplotlib in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (3.4.3)\n",
      "Requirement already satisfied: absl-py==0.7.1 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (0.7.1)\n",
      "Requirement already satisfied: dm-sonnet==1.34 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (1.34)\n",
      "Requirement already satisfied: numpy==1.16.4 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (1.16.4)\n",
      "Requirement already satisfied: Pillow in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (9.5.0)\n",
      "Requirement already satisfied: tensorflow==1.15 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (1.15.0)\n",
      "Requirement already satisfied: tensorflow-probability==0.7.0 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (0.7.0)\n",
      "Requirement already satisfied: tensorflow-gan==2.0.0 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (2.0.0)\n",
      "Requirement already satisfied: protobuf==3.20.3 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (3.20.3)\n",
      "Requirement already satisfied: six in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from absl-py==0.7.1) (1.16.0)\n",
      "Requirement already satisfied: contextlib2 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from dm-sonnet==1.34) (21.6.0)\n",
      "Requirement already satisfied: wrapt in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from dm-sonnet==1.34) (1.16.0)\n",
      "Requirement already satisfied: semantic-version in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from dm-sonnet==1.34) (2.10.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from tensorflow==1.15) (1.62.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from tensorflow==1.15) (2.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from tensorflow==1.15) (1.15.1)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from tensorflow==1.15) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from tensorflow==1.15) (1.15.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from tensorflow==1.15) (0.8.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from tensorflow==1.15) (1.0.8)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from tensorflow==1.15) (0.41.3)\n",
      "Requirement already satisfied: cloudpickle>=0.6.1 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from tensorflow-probability==0.7.0) (2.2.1)\n",
      "Requirement already satisfied: decorator in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from tensorflow-probability==0.7.0) (5.1.1)\n",
      "Requirement already satisfied: tensorflow-hub>=0.2 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from tensorflow-gan==2.0.0) (0.16.0)\n",
      "Requirement already satisfied: pooch>=1.0 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from librosa) (1.0.2)\n",
      "Requirement already satisfied: numba>=0.43.0 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from librosa) (0.53.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from librosa) (0.2.2)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from librosa) (23.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from ipywidgets) (7.33.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: pickleshare in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: matplotlib-inline in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: backcall in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (65.3.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: h5py in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (4.7.1)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from numba>=0.43.0->librosa) (0.36.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from pooch>=1.0->librosa) (4.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.2.3)\n",
      "Requirement already satisfied: pycparser in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (6.7.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2025.8.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa scipy ipywidgets matplotlib absl-py==0.7.1 dm-sonnet==1.34 numpy==1.16.4 Pillow tensorflow==1.15 tensorflow-probability==0.7.0 tensorflow-gan==2.0.0 protobuf==3.20.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb9443dd-446a-4f3f-9c57-11593459ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7b5f178-4cc7-4943-8427-4830936a98da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import scipy.io.wavfile as wav\n",
    "from os.path import isfile, join\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "def create_specs(fft, time_long, step_size, log_ref):\n",
    "  audio_dir='/home/studio-lab-user/sagemaker-studiolab-notebooks/free-spoken-digit-dataset/recordings/'\n",
    "  file_names = [f for f in listdir(audio_dir) if isfile(join(audio_dir, f)) and '.wav' in f]\n",
    "  ms_list = np.zeros([0,int(fft/2),int(time_long/step_size)])\n",
    "\n",
    "  #sp_sz=2046\n",
    "  sp_sz = int(time_long)\n",
    "  i = 0\n",
    "  for file_name in file_names:\n",
    "    i += 1\n",
    "    audio_path = audio_dir + file_name\n",
    "\n",
    "    sample_rate, samples = wav.read(audio_path)\n",
    "    samples = np.append(samples, np.random.randn(sp_sz-samples.shape[0]%sp_sz)*10, axis=0)\n",
    "    ms = np.transpose(pretty_spectrogram(samples.astype(\"float32\"),fft_size=fft,step_size=step_size,log=False))\n",
    "    n_ms = samples.shape[0]//sp_sz\n",
    "    ms = np.expand_dims(librosa.power_to_db(ms,\n",
    "                                            ref=log_ref), axis=0)\n",
    "    lms = np.split(ms, n_ms, axis=2)\n",
    "    ms2 = np.concatenate(lms)\n",
    "    ms_list = np.append(ms_list,ms2,axis=0)\n",
    "  clip = float(np.ceil(np.amax(ms_list)))\n",
    "  print(\"clip\",clip)\n",
    "  print(\"min value spec\", np.amin(ms_list))\n",
    "  print(\"max value spec\",np.amax(ms_list))\n",
    "  X_train, X_test = train_test_split(\n",
    "    ms_list, test_size=0.20, random_state=42)\n",
    "  return X_train, X_test, clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ec87c6a-6c12-4fd5-aa01-0f9a03991312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title biblio de audio\n",
    "\n",
    "import IPython.display\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "\n",
    "# Packages we're using\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import butter, lfilter\n",
    "import scipy.ndimage\n",
    "# Most of the Spectrograms and Inversion are taken from: https://gist.github.com/kastnerkyle/179d6e9a88202ab0a2fe\n",
    "\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype=\"band\")\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "def overlap(X, window_size, window_step):\n",
    "    \"\"\"\n",
    "    Create an overlapped version of X\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape=(n_samples,)\n",
    "        Input signal to window and overlap\n",
    "    window_size : int\n",
    "        Size of windows to take\n",
    "    window_step : int\n",
    "        Step size between windows\n",
    "    Returns\n",
    "    -------\n",
    "    X_strided : shape=(n_windows, window_size)\n",
    "        2D array of overlapped X\n",
    "    \"\"\"\n",
    "    if window_size % 2 != 0:\n",
    "        raise ValueError(\"Window size must be even!\")\n",
    "    # Make sure there are an even number of windows before stridetricks\n",
    "    append = np.zeros((window_size - len(X) % window_size))\n",
    "    X = np.hstack((X, append))\n",
    "\n",
    "    ws = window_size\n",
    "    ss = window_step\n",
    "    a = X\n",
    "\n",
    "    valid = len(a) - ws\n",
    "    nw = (valid) // ss\n",
    "    out = np.ndarray((nw, ws), dtype=a.dtype)\n",
    "\n",
    "    for i in np.arange(nw):\n",
    "        # \"slide\" the window along the samples\n",
    "        start = i * ss\n",
    "        stop = start + ws\n",
    "        out[i] = a[start:stop]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def stft(\n",
    "    X, fftsize=128, step=65, mean_normalize=True, real=False, compute_onesided=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute STFT for 1D real valued input X\n",
    "    \"\"\"\n",
    "    if real:\n",
    "        local_fft = np.fft.rfft\n",
    "        cut = -1\n",
    "    else:\n",
    "        local_fft = np.fft.fft\n",
    "        cut = None\n",
    "    if compute_onesided:\n",
    "        cut = fftsize // 2\n",
    "    if mean_normalize:\n",
    "        X -= X.mean()\n",
    "\n",
    "    X = overlap(X, fftsize, step)\n",
    "\n",
    "    size = fftsize\n",
    "    win = 0.54 - 0.46 * np.cos(2 * np.pi * np.arange(size) / (size - 1))\n",
    "    X = X * win[None]\n",
    "    X = local_fft(X)[:, :cut]\n",
    "    return X\n",
    "\n",
    "\n",
    "def pretty_spectrogram(d, log=True, thresh=5, fft_size=512, step_size=64):\n",
    "    \"\"\"\n",
    "    creates a spectrogram\n",
    "    log: take the log of the spectrgram\n",
    "    thresh: threshold minimum power for log spectrogram\n",
    "    \"\"\"\n",
    "    specgram = np.abs(\n",
    "        stft(d, fftsize=fft_size, step=step_size, real=False, compute_onesided=True)\n",
    "    )\n",
    "\n",
    "    if log == True:\n",
    "        specgram /= specgram.max()  # volume normalize to max 1\n",
    "        specgram = np.log10(specgram)  # take log\n",
    "        specgram[\n",
    "            specgram < -thresh\n",
    "        ] = -thresh  # set anything less than the threshold as the threshold\n",
    "    else:\n",
    "        specgram[\n",
    "            specgram < thresh\n",
    "        ] = thresh  # set anything less than the threshold as the threshold\n",
    "\n",
    "    return specgram\n",
    "\n",
    "\n",
    "# Also mostly modified or taken from https://gist.github.com/kastnerkyle/179d6e9a88202ab0a2fe\n",
    "def invert_pretty_spectrogram(\n",
    "    X_s, log=True, fft_size=512, step_size=512 / 4, n_iter=10\n",
    "):\n",
    "\n",
    "    if log == True:\n",
    "        X_s = np.power(10, X_s)\n",
    "\n",
    "    X_s = np.concatenate([X_s, X_s[:, ::-1]], axis=1)\n",
    "    X_t = iterate_invert_spectrogram(X_s, fft_size, step_size, n_iter=n_iter)\n",
    "    return X_t\n",
    "\n",
    "\n",
    "def iterate_invert_spectrogram(X_s, fftsize, step, n_iter=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Under MSR-LA License\n",
    "    Based on MATLAB implementation from Spectrogram Inversion Toolbox\n",
    "    References\n",
    "    ----------\n",
    "    D. Griffin and J. Lim. Signal estimation from modified\n",
    "    short-time Fourier transform. IEEE Trans. Acoust. Speech\n",
    "    Signal Process., 32(2):236-243, 1984.\n",
    "    Malcolm Slaney, Daniel Naar and Richard F. Lyon. Auditory\n",
    "    Model Inversion for Sound Separation. Proc. IEEE-ICASSP,\n",
    "    Adelaide, 1994, II.77-80.\n",
    "    Xinglei Zhu, G. Beauregard, L. Wyse. Real-Time Signal\n",
    "    Estimation from Modified Short-Time Fourier Transform\n",
    "    Magnitude Spectra. IEEE Transactions on Audio Speech and\n",
    "    Language Processing, 08/2007.\n",
    "    \"\"\"\n",
    "    reg = np.max(X_s) / 1e8\n",
    "    X_best = copy.deepcopy(X_s)\n",
    "    for i in range(n_iter):\n",
    "        if verbose:\n",
    "            print(\"Runnning iter %i\" % i)\n",
    "        if i == 0:\n",
    "            X_t = invert_spectrogram(\n",
    "                X_best, step, calculate_offset=True, set_zero_phase=True\n",
    "            )\n",
    "        else:\n",
    "            # Calculate offset was False in the MATLAB version\n",
    "            # but in mine it massively improves the result\n",
    "            # Possible bug in my impl?\n",
    "            X_t = invert_spectrogram(\n",
    "                X_best, step, calculate_offset=True, set_zero_phase=False\n",
    "            )\n",
    "        est = stft(X_t, fftsize=fftsize, step=step, compute_onesided=False)\n",
    "        phase = est / np.maximum(reg, np.abs(est))\n",
    "        X_best = X_s * phase[: len(X_s)]\n",
    "    X_t = invert_spectrogram(X_best, step, calculate_offset=True, set_zero_phase=False)\n",
    "    return np.real(X_t)\n",
    "\n",
    "\n",
    "def invert_spectrogram(X_s, step, calculate_offset=True, set_zero_phase=True):\n",
    "    \"\"\"\n",
    "    Under MSR-LA License\n",
    "    Based on MATLAB implementation from Spectrogram Inversion Toolbox\n",
    "    References\n",
    "    ----------\n",
    "    D. Griffin and J. Lim. Signal estimation from modified\n",
    "    short-time Fourier transform. IEEE Trans. Acoust. Speech\n",
    "    Signal Process., 32(2):236-243, 1984.\n",
    "    Malcolm Slaney, Daniel Naar and Richard F. Lyon. Auditory\n",
    "    Model Inversion for Sound Separation. Proc. IEEE-ICASSP,\n",
    "    Adelaide, 1994, II.77-80.\n",
    "    Xinglei Zhu, G. Beauregard, L. Wyse. Real-Time Signal\n",
    "    Estimation from Modified Short-Time Fourier Transform\n",
    "    Magnitude Spectra. IEEE Transactions on Audio Speech and\n",
    "    Language Processing, 08/2007.\n",
    "    \"\"\"\n",
    "    size = int(X_s.shape[1] // 2)\n",
    "    wave = np.zeros((X_s.shape[0] * step + size))\n",
    "    # Getting overflow warnings with 32 bit...\n",
    "    wave = wave.astype(\"float64\")\n",
    "    total_windowing_sum = np.zeros((X_s.shape[0] * step + size))\n",
    "    win = 0.54 - 0.46 * np.cos(2 * np.pi * np.arange(size) / (size - 1))\n",
    "\n",
    "    est_start = int(size // 2) - 1\n",
    "    est_end = est_start + size\n",
    "    for i in range(X_s.shape[0]):\n",
    "        wave_start = int(step * i)\n",
    "        wave_end = wave_start + size\n",
    "        if set_zero_phase:\n",
    "            spectral_slice = X_s[i].real + 0j\n",
    "        else:\n",
    "            # already complex\n",
    "            spectral_slice = X_s[i]\n",
    "\n",
    "        # Don't need fftshift due to different impl.\n",
    "        wave_est = np.real(np.fft.ifft(spectral_slice))[::-1]\n",
    "        if calculate_offset and i > 0:\n",
    "            offset_size = size - step\n",
    "            if offset_size <= 0:\n",
    "                print(\n",
    "                    \"WARNING: Large step size >50\\% detected! \"\n",
    "                    \"This code works best with high overlap - try \"\n",
    "                    \"with 75% or greater\"\n",
    "                )\n",
    "                offset_size = step\n",
    "            offset = xcorr_offset(\n",
    "                wave[wave_start : wave_start + offset_size],\n",
    "                wave_est[est_start : est_start + offset_size],\n",
    "            )\n",
    "        else:\n",
    "            offset = 0\n",
    "        wave[wave_start:wave_end] += (\n",
    "            win * wave_est[est_start - offset : est_end - offset]\n",
    "        )\n",
    "        total_windowing_sum[wave_start:wave_end] += win\n",
    "    wave = np.real(wave) / (total_windowing_sum + 1e-6)\n",
    "    return wave\n",
    "\n",
    "def xcorr_offset(x1, x2):\n",
    "    \"\"\"\n",
    "    Under MSR-LA License\n",
    "    Based on MATLAB implementation from Spectrogram Inversion Toolbox\n",
    "    References\n",
    "    ----------\n",
    "    D. Griffin and J. Lim. Signal estimation from modified\n",
    "    short-time Fourier transform. IEEE Trans. Acoust. Speech\n",
    "    Signal Process., 32(2):236-243, 1984.\n",
    "    Malcolm Slaney, Daniel Naar and Richard F. Lyon. Auditory\n",
    "    Model Inversion for Sound Separation. Proc. IEEE-ICASSP,\n",
    "    Adelaide, 1994, II.77-80.\n",
    "    Xinglei Zhu, G. Beauregard, L. Wyse. Real-Time Signal\n",
    "    Estimation from Modified Short-Time Fourier Transform\n",
    "    Magnitude Spectra. IEEE Transactions on Audio Speech and\n",
    "    Language Processing, 08/2007.\n",
    "    \"\"\"\n",
    "    x1 = x1 - x1.mean()\n",
    "    x2 = x2 - x2.mean()\n",
    "    frame_size = len(x2)\n",
    "    half = frame_size // 2\n",
    "    corrs = np.convolve(x1.astype(\"float32\"), x2[::-1].astype(\"float32\"))\n",
    "    corrs[:half] = -1e30\n",
    "    corrs[-half:] = -1e30\n",
    "    offset = corrs.argmax() - len(x1)\n",
    "    return offset\n",
    "\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "### Parameters ###\n",
    "fft_size = 512  # window size for the FFT\n",
    "step_size = fft_size // 16  # distance to slide along the window (in time)\n",
    "spec_thresh = 4  # threshold for spectrograms (lower filters out more noise)\n",
    "lowcut = 500  # Hz # Low cut for our butter bandpass filter\n",
    "highcut = 4000  # Hz # High cut for our butter bandpass filter\n",
    "# For mels\n",
    "n_mel_freq_components = 64  # number of mel frequency channels\n",
    "shorten_factor = 10  # how much should we compress the x-axis (time)\n",
    "start_freq = 50  # Hz # What frequency to start sampling our melS from\n",
    "end_freq = 4000\n",
    "audio_path='/home/studio-lab-user/sagemaker-studiolab-notebooks/free-spoken-digit-dataset/recordings/0_george_0.wav'\n",
    "data_rate, data = wav.read(audio_path)\n",
    "wav_spectrogram = pretty_spectrogram(\n",
    "data.astype(\"float64\"),\n",
    "fft_size=fft_size,\n",
    "step_size=step_size,\n",
    "log=True,\n",
    "thresh=spec_thresh,\n",
    ")\n",
    "\n",
    "# Invert from the spectrogram back to a waveform\n",
    "recovered_audio_orig = invert_pretty_spectrogram(\n",
    "    wav_spectrogram, fft_size=fft_size, step_size=step_size, log=True, n_iter=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33de55af-d1b3-4fdd-b081-02d6926e4ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip 55.0\n",
      "min value spec 0.0\n",
      "max value spec 54.80099098502209\n",
      "number of measures 128\n"
     ]
    }
   ],
   "source": [
    "fft = 256\n",
    "time_long = 512\n",
    "step_size = 32\n",
    "fft_step_size_ratio = int(fft/step_size)\n",
    "log_ref = 5e-0\n",
    "##put . to std to make it float\n",
    "std=0.0\n",
    "compression_ratio = 0.25\n",
    "measures = int(compression_ratio * time_long)\n",
    "CRF='DCS25'\n",
    "X_train, X_test, clip = create_specs(fft=fft, time_long=time_long, step_size= step_size, log_ref= log_ref)\n",
    "print('number of measures', measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4c672d5-66a2-4ca8-bbac-468b076872b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0908 14:02:59.746343 140566818527040 module_wrapper.py:139] From /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages/sonnet/python/custom_getters/restore_initializer.py:27: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W0908 14:02:59.753797 140566818527040 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0908 14:03:00.139117 140566818527040 module_wrapper.py:139] From /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages/tensorflow_gan/python/estimator/tpu_gan_estimator.py:42: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.\n",
      "\n",
      "W0908 14:03:00.145645 140566818527040 module_wrapper.py:139] From /tmp/ipykernel_265/454366551.py:90: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "W0908 14:03:00.151690 140566818527040 module_wrapper.py:139] From /tmp/ipykernel_265/454366551.py:90: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages/ipykernel_launcher.py']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import sys\n",
    "#sys.path.append('/content/deepmind-research')\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "sys.path.append('/content/')\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "####Delete all flags before declare#####\n",
    "'''\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(flags.FLAGS)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Copyright 2019 DeepMind Technologies Limited and Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Training script.\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from cs_gan import cs\n",
    "from cs_gan import file_utils\n",
    "from cs_gan import utils\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'mode', 'recons', 'Model mode.')\n",
    "flags.DEFINE_integer(\n",
    "    'num_training_iterations', 100000,\n",
    "    'Number of training iterations.')\n",
    "flags.DEFINE_integer(\n",
    "    'batch_size', 64, 'Training batch size.')\n",
    "flags.DEFINE_integer(\n",
    "    'num_measurements', measures, 'The number of measurements')\n",
    "flags.DEFINE_integer(\n",
    "    'num_latents', 100, 'The number of latents')\n",
    "flags.DEFINE_integer(\n",
    "    'num_z_iters', 3, 'The number of latent optimisation steps.')\n",
    "flags.DEFINE_float(\n",
    "    'z_step_size', 0.01, 'Step size for latent optimisation.')\n",
    "flags.DEFINE_string(\n",
    "    'z_project_method', 'norm', 'The method to project z.')\n",
    "flags.DEFINE_integer(\n",
    "    'summary_every_step', 10000,\n",
    "    'The interval at which to log debug ops.')\n",
    "flags.DEFINE_integer(\n",
    "    'export_every', 10,\n",
    "    'The interval at which to export samples.')\n",
    "flags.DEFINE_string(\n",
    "    'dataset', 'mnist', 'The dataset used for learning (cifar|mnist.')\n",
    "flags.DEFINE_float('learning_rate', 1e-4, 'Learning rate.')\n",
    "flags.DEFINE_string(\n",
    "    'output_dir','/gdrive/My Drive/'+CRF, 'Location where to save output files.')\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Log info level (for Hooks).\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "FLAGS(sys.argv)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f909820-2515-465b-aaa1-1bd99e1ad9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17612, 128, 16)\n",
      "(17612, 1)\n",
      "(17612, 128, 16, 1) (4403, 128, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "temp = -np.ones((X_train.shape[0],1))\n",
    "print(temp.shape)\n",
    "X_train = np.reshape(X_train, [-1, X_train.shape[1], X_train.shape[2], 1])\n",
    "X_test = np.reshape(X_test, [-1, X_test.shape[1], X_test.shape[2], 1])\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04869bba-fd28-40bc-a5c2-40b6ed27a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class specData():\n",
    "  def __init__(self,fft,time_long, step_size, log_ref,clip):\n",
    "    self.fft= fft\n",
    "    self.time_long = time_long\n",
    "    self.fft_step_size_ratio = int(fft/step_size)\n",
    "    self.clip = clip\n",
    "    self.log_ref = log_ref\n",
    "    self.gen_output_shape = [int(fft/2), int(self.time_long / step_size) , 1]\n",
    "    self.gen_net_shape = [1000, 1000, self.gen_output_shape[0]* self.gen_output_shape[1] ]\n",
    "\n",
    "spec = specData(fft,time_long,step_size, log_ref, clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d42f638-5510-421f-8cd2-1e6df5be33e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 DeepMind Technologies Limited and Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"GAN modules.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "\n",
    "import sonnet as snt\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from cs_gan import utils\n",
    "\n",
    "\n",
    "class CS1(object):\n",
    "  \"\"\"Compressed Sensing Module.\"\"\"\n",
    "\n",
    "  def __init__(self, metric_net, generator,\n",
    "               num_z_iters, z_step_size, z_project_method):\n",
    "    \"\"\"Constructs the module.\n",
    "\n",
    "    Args:\n",
    "      metric_net: the measurement network.\n",
    "      generator: The generator network. A sonnet module. For examples, see\n",
    "        `nets.py`.\n",
    "      num_z_iters: an integer, the number of latent optimisation steps.\n",
    "      z_step_size: an integer, latent optimisation step size.\n",
    "      z_project_method: the method for projecting latent after optimisation,\n",
    "        a string from {'norm', 'clip'}.\n",
    "    \"\"\"\n",
    "\n",
    "    self._measure = metric_net\n",
    "    self.generator = generator\n",
    "    self.num_z_iters = num_z_iters\n",
    "    self.z_project_method = z_project_method\n",
    "    self._log_step_size_module = snt.TrainableVariable(\n",
    "        [],\n",
    "        initializers={'w': tf.constant_initializer(math.log(z_step_size))})\n",
    "    self.z_step_size = tf.exp(self._log_step_size_module())\n",
    "\n",
    "  def connect(self, data, generator_inputs, std):\n",
    "    \"\"\"Connects the components and returns the losses, outputs and debug ops.\n",
    "\n",
    "    Args:\n",
    "      data: a `tf.Tensor`: `[batch_size, ...]`. There are no constraints on the\n",
    "        rank\n",
    "        of this tensor, but it has to be compatible with the shapes expected\n",
    "        by the discriminator.\n",
    "      generator_inputs: a `tf.Tensor`: `[g_in_batch_size, ...]`. It does not\n",
    "        have to have the same batch size as the `data` tensor. There are not\n",
    "        constraints on the rank of this tensor, but it has to be compatible\n",
    "        with the shapes the generator network supports as inputs.\n",
    "\n",
    "    Returns:\n",
    "      An `ModelOutputs` instance.\n",
    "    \"\"\"\n",
    "\n",
    "    samples, optimised_z = utils.optimise_and_sample(\n",
    "        generator_inputs, self, data, std, is_training=True)\n",
    "    optimisation_cost = utils.get_optimisation_cost(generator_inputs,\n",
    "                                                    optimised_z)\n",
    "    debug_ops = {}\n",
    "\n",
    "    initial_samples = self.generator(generator_inputs, is_training=True)\n",
    "    eps = tf.random_normal(tf.shape(self._measure(data)), 0, 1, dtype=tf.float32)\n",
    "    ruido = tf.multiply(0.0, eps)\n",
    "    generator_loss = tf.reduce_mean(self.gen_loss_fn(data, samples, ruido))\n",
    "    # compute the RIP loss\n",
    "    # (\\sqrt{F(x_1 - x_2)^2} - \\sqrt{(x_1 - x_2)^2})^2\n",
    "    # as a triplet loss for 3 pairs of images.\n",
    "\n",
    "    r1 = self._get_rip_loss(samples, initial_samples)\n",
    "    r2 = self._get_rip_loss(samples, data)\n",
    "    r3 = self._get_rip_loss(initial_samples, data)\n",
    "    rip_loss = tf.reduce_mean((r1 + r2 + r3) / 3.0)\n",
    "    total_loss = generator_loss + rip_loss\n",
    "    optimization_components = self._build_optimization_components(\n",
    "        generator_loss=total_loss)\n",
    "    debug_ops['rip_loss'] = rip_loss\n",
    "    debug_ops['recons_loss'] = tf.reduce_mean(\n",
    "        tf.norm(snt.BatchFlatten()(samples)\n",
    "                - snt.BatchFlatten()(data), axis=-1))\n",
    "    debug_ops['recons_loss'] = tf.reduce_mean(\n",
    "        tf.norm(snt.BatchFlatten()(samples)\n",
    "                - snt.BatchFlatten()(data), axis=-1))/4096\n",
    "    debug_ops['z_step_size'] = self.z_step_size\n",
    "    debug_ops['opt_cost'] = optimisation_cost\n",
    "    debug_ops['gen_loss'] = generator_loss\n",
    "\n",
    "    return utils.ModelOutputs(\n",
    "        optimization_components, debug_ops)\n",
    "\n",
    "  def _get_rip_loss(self, img1, img2):\n",
    "    r\"\"\"Compute the RIP loss from two images.\n",
    "\n",
    "      The RIP loss: (\\sqrt{F(x_1 - x_2)^2} - \\sqrt{(x_1 - x_2)^2})^2\n",
    "\n",
    "    Args:\n",
    "      img1: an image (x_1), 4D tensor of shape [batch_size, W, H, C].\n",
    "      img2: an other image (x_2), 4D tensor of shape [batch_size, W, H, C].\n",
    "    \"\"\"\n",
    "\n",
    "    m1 = self._measure(img1)\n",
    "    m2 = self._measure(img2)\n",
    "\n",
    "    img_diff_norm = tf.norm(snt.BatchFlatten()(img1)\n",
    "                            - snt.BatchFlatten()(img2), axis=-1)\n",
    "    m_diff_norm = tf.norm(m1 - m2, axis=-1)\n",
    "\n",
    "    return tf.square(img_diff_norm - m_diff_norm)\n",
    "\n",
    "  def _get_measurement_error(self, target_img, sample_img, ruido):\n",
    "    \"\"\"Compute the measurement error of sample images given the targets.\"\"\"\n",
    "\n",
    "    m_targets = self._measure(target_img)\n",
    "    m_targets_con_ruido = tf.add(m_targets, ruido)\n",
    "    m_samples = self._measure(sample_img)\n",
    "\n",
    "    return tf.reduce_sum(tf.square(m_targets_con_ruido - m_samples), -1)\n",
    "\n",
    "  def gen_loss_fn(self, data, samples, ruido):\n",
    "    \"\"\"Generator loss as latent optimisation's error function.\"\"\"\n",
    "    return self._get_measurement_error(data, samples, ruido)\n",
    "\n",
    "  def _build_optimization_components(\n",
    "      self, generator_loss=None, discriminator_loss=None):\n",
    "    \"\"\"Create the optimization components for this module.\"\"\"\n",
    "\n",
    "    metric_vars = _get_and_check_variables(self._measure)\n",
    "    generator_vars = _get_and_check_variables(self.generator)\n",
    "    step_vars = _get_and_check_variables(self._log_step_size_module)\n",
    "\n",
    "    assert discriminator_loss is None\n",
    "    optimization_components = utils.OptimizationComponent(\n",
    "        generator_loss, generator_vars + metric_vars + step_vars)\n",
    "    return optimization_components\n",
    "\n",
    "\n",
    "def _get_and_check_variables(module):\n",
    "  module_variables = module.get_all_variables()\n",
    "  if not module_variables:\n",
    "    raise ValueError(\n",
    "        'Module {} has no variables! Variables needed for training.'.format(\n",
    "            module.module_name))\n",
    "\n",
    "  # TensorFlow optimizers require lists to be passed in.\n",
    "  return list(module_variables)\n",
    "class MLPGeneratorNet(snt.AbstractModule):\n",
    "  \"\"\"MNIST generator net.\"\"\"\n",
    "\n",
    "  def __init__(self, name='mlp_generator'):\n",
    "    super(MLPGeneratorNet, self).__init__(name=name)\n",
    "\n",
    "  def _build(self, inputs, is_training=True):\n",
    "    del is_training\n",
    "    #net = snt.nets.MLP([1000, 1000, 4096], activation=tf.nn.leaky_relu)\n",
    "    net = snt.nets.MLP(spec.gen_net_shape, activation=tf.nn.leaky_relu)\n",
    "    out = net(inputs)\n",
    "    out = tf.nn.tanh(out)\n",
    "    #return snt.BatchReshape([256, 16, 1])(out)\n",
    "    return snt.BatchReshape(spec.gen_output_shape)(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eda15a3d-4fea-4957-a89d-47c5095e7377",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "486254b7-788d-4cc3-99c8-f05534079b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-15cb3fe683ff3f7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-15cb3fe683ff3f7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f420ea1a-a332-493d-b637-fa3a494c218e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0908 14:03:45.216815 140566818527040 deprecation.py:323] From /tmp/ipykernel_265/1456660690.py:36: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "W0908 14:03:47.337588 140566818527040 module_wrapper.py:139] From /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages/sonnet/python/modules/base.py:177: The name tf.make_template is deprecated. Please use tf.compat.v1.make_template instead.\n",
      "\n",
      "W0908 14:03:47.360165 140566818527040 module_wrapper.py:139] From /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages/sonnet/python/modules/base.py:278: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0908 14:03:47.361554 140566818527040 module_wrapper.py:139] From /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages/sonnet/python/modules/basic.py:993: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0908 14:03:47.419397 140566818527040 module_wrapper.py:139] From /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages/sonnet/python/modules/base.py:579: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0908 14:03:47.431184 140566818527040 deprecation.py:506] From /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages/sonnet/python/modules/basic.py:126: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0908 14:03:47.432271 140566818527040 deprecation.py:506] From /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages/sonnet/python/modules/basic.py:131: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0908 14:03:48.195714 140566818527040 module_wrapper.py:139] From /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages/sonnet/python/modules/base.py:693: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "W0908 14:03:49.138676 140566818527040 deprecation.py:323] From /home/studio-lab-user/.conda/envs/dcs2/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "I0908 14:03:49.853811 140566818527040 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n"
     ]
    }
   ],
   "source": [
    "utils.make_output_dir('/home/studio-lab-user/sagemaker-studiolab-notebooks/checkpoint/'+CRF)\n",
    "data_processor = utils.DataProcessor()\n",
    "\n",
    "\n",
    "def get_np_data(data_processor, dataset, split='train'):\n",
    "  \"\"\"Get the dataset as numpy arrays.\"\"\"\n",
    "\n",
    "  if split == 'train':\n",
    "    # Construct the dataset.\n",
    "    x = X_train\n",
    "    # Note: tf dataset is binary so we convert it to float.\n",
    "    x = x.astype(np.float32)\n",
    "    x = x / spec.clip\n",
    "\n",
    "  if split == 'valid':\n",
    "    x =  X_test\n",
    "    x = x.astype(np.float32)\n",
    "    x = x / spec.clip\n",
    "\n",
    "  if data_processor:\n",
    "    # Normalize data if a processor is given.\n",
    "    x = data_processor.preprocess(x)\n",
    "  return x\n",
    "def get_train_dataset2(data_processor, dataset, batch_size):\n",
    "  \"\"\"Creates the training data tensors.\"\"\"\n",
    "  x_train = get_np_data(data_processor, dataset, split='train')\n",
    "  # Create the TF dataset.\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "\n",
    "  # Shuffle and repeat the dataset for training.\n",
    "  # This is required because we want to do multiple passes through the entire\n",
    "  # dataset when training.\n",
    "  dataset = dataset.shuffle(100000).repeat()\n",
    "\n",
    "  # Batch the data and return the data batch.\n",
    "  one_shot_iterator = dataset.batch(batch_size).make_one_shot_iterator()\n",
    "  data_batch = one_shot_iterator.get_next()\n",
    "  return data_batch\n",
    "\n",
    "def get_real_data_for_eval2(num_eval_samples, dataset, split='valid'):\n",
    "  data = get_np_data(data_processor=None, dataset=dataset, split=split)\n",
    "  data = data[:num_eval_samples]\n",
    "  return tf.constant(data)\n",
    "images1=utils._get_np_data(data_processor, FLAGS.dataset, split='train')\n",
    "print(images1.shape)\n",
    "\n",
    "images = get_train_dataset2(data_processor, FLAGS.dataset,\n",
    "                                  FLAGS.batch_size)\n",
    "\n",
    "logging.info('Learning rate: %d', FLAGS.learning_rate)\n",
    "\n",
    "# Construct optimizers.\n",
    "optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "\n",
    "# Create the networks and models.\n",
    "generator = MLPGeneratorNet(FLAGS.dataset)\n",
    "#generator = utils.get_generator(FLAGS.dataset)\n",
    "metric_net = utils.get_metric_net(FLAGS.dataset, FLAGS.num_measurements)\n",
    "model = CS1(metric_net, generator,\n",
    "              FLAGS.num_z_iters, FLAGS.z_step_size, FLAGS.z_project_method)\n",
    "prior = utils.make_prior(FLAGS.num_latents)\n",
    "generator_inputs = prior.sample(FLAGS.batch_size)\n",
    "\n",
    "std=10.0\n",
    "model_output = model.connect(images, generator_inputs, std)\n",
    "optimization_components = model_output.optimization_components\n",
    "debug_ops = model_output.debug_ops\n",
    "\n",
    "reconstructions, _ = utils.optimise_and_sample(\n",
    "    generator_inputs, model, images, std, is_training=False)\n",
    "images2 = tf.placeholder(tf.float32, shape=(64, spec.gen_output_shape[0], spec.gen_output_shape[1], 1))\n",
    "reconstructionsa, _ = utils.optimise_and_sample(\n",
    "  generator_inputs, model, images2,std,  is_training=False)\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "update_op = optimizer.minimize(\n",
    "    optimization_components.loss,\n",
    "    var_list=optimization_components.vars,\n",
    "    global_step=global_step)\n",
    "\n",
    "sample_exporter = file_utils.FileExporter(\n",
    "    os.path.join('/home/studio-lab-user/sagemaker-studiolab-notebooks/checkpoint/'+CRF, 'reconstructions'))\n",
    "\n",
    "# Hooks.\n",
    "debug_ops['it'] = global_step\n",
    "# Abort training on Nans.\n",
    "nan_hook = tf.train.NanTensorHook(optimization_components.loss)\n",
    "# Step counter.\n",
    "\n",
    "\n",
    "checkpoint_saver_hook = tf.train.CheckpointSaverHook(\n",
    "    checkpoint_dir='/home/studio-lab-user/sagemaker-studiolab-notebooks/checkpoint/'+CRF, save_steps=100000)\n",
    "\n",
    "loss_summary_saver_hook = tf.train.SummarySaverHook(\n",
    "    output_dir='/home/studio-lab-user/sagemaker-studiolab-notebooks/checkpoint/'+CRF,\n",
    "    save_steps=5000,\n",
    "    summary_op=utils.get_summaries(debug_ops))\n",
    "\n",
    "hooks = [checkpoint_saver_hook, nan_hook, loss_summary_saver_hook]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb859e68-5064-4ba9-91a2-b884e3a42998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it [10100]\n",
      "z_step_size [0.00657674577]\n",
      "opt_cost [100.396896]\n",
      "recons_loss [0.00115100236]\n",
      "gen_loss [1.42176127]\n",
      "rip_loss [4.24925089]\n",
      "z_step_size [0.00653786771]\n",
      "it [10200]\n",
      "opt_cost [96.7141113]\n",
      "recons_loss [0.0011322191]\n",
      "gen_loss [1.34456801]\n",
      "rip_loss [4.08750343]\n",
      "it [10300]\n",
      "z_step_size [0.00650236243]\n",
      "opt_cost [100.076431]\n",
      "recons_loss [0.0011378444]\n",
      "gen_loss [1.41349697]\n",
      "rip_loss [4.08294153]\n",
      "it [10400]\n",
      "z_step_size [0.00646156073]\n",
      "opt_cost [99.1120911]\n",
      "recons_loss [0.00115863734]\n",
      "gen_loss [1.41787136]\n",
      "rip_loss [4.33518791]\n",
      "it [10500]\n",
      "z_step_size [0.00642602844]\n",
      "opt_cost [98.2628784]\n",
      "recons_loss [0.0011413]\n",
      "rip_loss [4.18857765]\n",
      "gen_loss [1.33865726]\n",
      "it [10600]\n",
      "z_step_size [0.00639024703]\n",
      "opt_cost [98.6155396]\n",
      "recons_loss [0.00112214475]\n",
      "gen_loss [1.36333907]\n",
      "rip_loss [4.00278664]\n",
      "it [10700]\n",
      "z_step_size [0.00635891175]\n",
      "opt_cost [101.09256]\n",
      "recons_loss [0.00111240335]\n",
      "gen_loss [1.33482635]\n",
      "rip_loss [3.94307852]\n",
      "z_step_size [0.00632436341]\n",
      "it [10800]\n",
      "opt_cost [99.9594193]\n",
      "recons_loss [0.00112901931]\n",
      "gen_loss [1.34258115]\n",
      "rip_loss [4.06691456]\n",
      "z_step_size [0.00629219553]\n",
      "it [10900]\n",
      "opt_cost [94.9307785]\n",
      "recons_loss [0.00112364977]\n",
      "rip_loss [4.01588154]\n",
      "gen_loss [1.34190893]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#Start training.\n",
    "t0 = time.time()\n",
    "with tf.train.MonitoredTrainingSession(hooks=hooks,save_checkpoint_secs=None,\n",
    "                                       save_checkpoint_steps=None,\n",
    "                                       save_summaries_steps=100,log_step_count_steps=None,checkpoint_dir='/home/studio-lab-user/sagemaker-studiolab-notebooks/checkpoint/'+CRF) as sess:\n",
    "\n",
    "  #train_writer = tf.summary.FileWriter( './logs/1/train ', sess.graph)\n",
    "  logging.info('starting training')\n",
    "  for i in range(FLAGS.num_training_iterations):\n",
    "    sess.run(update_op)\n",
    "    if i % 2000 == 0:\n",
    "      clear_output()\n",
    "\n",
    "\n",
    "\n",
    "    if i % FLAGS.export_every == 0:\n",
    "      reconstructions_np, data_np = sess.run([reconstructions, images])\n",
    "      # Create an object which gets data and does the processing.\n",
    "      data_np = data_processor.postprocess(data_np)\n",
    "      reconstructions_np = data_processor.postprocess(reconstructions_np)\n",
    "      sample_exporter.save(reconstructions_np, 'reconstructions')\n",
    "      sample_exporter.save(data_np, 'data')\n",
    "\n",
    "\n",
    "print( 'time: {}s'.  format(int(time.time()-t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40cd64a-148b-48ce-bcab-d188c0199e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c4633-366f-4c45-adac-635dfa9514cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcs2:Python",
   "language": "python",
   "name": "conda-env-dcs2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
